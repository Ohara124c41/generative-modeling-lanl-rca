{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ad2b26",
   "metadata": {},
   "source": [
    "# Project 5: Generative AI Applications (P5)\n",
    "\n",
    "Author: Christopher Aaron O'Hara\n",
    "\n",
    "Dataset target: LANL authentication event stream (public CSR datasets)\n",
    "- https://csr.lanl.gov/data/auth/\n",
    "- https://csr.lanl.gov/data/cyber1/\n",
    "\n",
    "Task: Transformer-based event sequence generation with RCA-oriented narrative interpretation.\n",
    "\n",
    "This notebook is intentionally independent of P6. It produces structured generated artifacts that can be integrated later in P7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7844e3",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "IIoT and local-cloud operations teams often receive high-volume event streams but limited analyst time. A compact generative model can produce candidate event narratives for stress testing and analyst support, as long as generated outputs are clearly labeled, quality-checked, and never treated as ground truth.\n",
    "\n",
    "This project focuses on a defensible generative workflow: train a Transformer language model on real LANL-style authentication event sequences, generate multiple samples, evaluate quality and failure modes, and derive RCA-style hypotheses from generated behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bdf30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /usr/bin/python3\n",
      "Torch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "GPU 0: Tesla T4\n",
      "  Total VRAM (GB): 14.56\n",
      "  Compute capability: 7.5\n",
      "\n",
      "=== nvidia-smi ===\n",
      "Fri Feb 20 05:35:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   57C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Colab/Server GPU check (T4 expected)\n",
    "import os\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "print(\"Python executable:\", os.sys.executable)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name}\")\n",
    "        print(f\"  Total VRAM (GB): {props.total_memory / (1024**3):.2f}\")\n",
    "        print(f\"  Compute capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"No CUDA GPU visible to this kernel.\")\n",
    "\n",
    "print(\"\\n=== nvidia-smi ===\")\n",
    "try:\n",
    "    out = subprocess.check_output([\"nvidia-smi\"], stderr=subprocess.STDOUT, text=True)\n",
    "    print(out[:2000])  # truncate for notebook readability\n",
    "except Exception as e:\n",
    "    print(\"nvidia-smi not available or failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c394c",
   "metadata": {},
   "source": [
    "## 1) Setup and Dependency Bootstrap\n",
    "\n",
    "This cell installs missing core dependencies for a clean environment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f4907bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap status: {'missing_before_install': [], 'installed_now': []}\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    'numpy': 'numpy',\n",
    "    'pandas': 'pandas',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    'torch': 'torch',\n",
    "    'networkx': 'networkx',\n",
    "}\n",
    "\n",
    "\n",
    "def ensure_packages(required):\n",
    "    missing = []\n",
    "    for import_name, pip_name in required.items():\n",
    "        if importlib.util.find_spec(import_name) is None:\n",
    "            missing.append(pip_name)\n",
    "\n",
    "    status = {'missing_before_install': missing.copy(), 'installed_now': []}\n",
    "    if missing:\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', '--quiet'] + missing\n",
    "        subprocess.check_call(cmd)\n",
    "        status['installed_now'] = missing\n",
    "    return status\n",
    "\n",
    "\n",
    "bootstrap_status = ensure_packages(REQUIRED_PACKAGES)\n",
    "print('Bootstrap status:', bootstrap_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ade338",
   "metadata": {},
   "source": [
    "## 2) Imports and Reproducibility Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73565521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'device': 'cuda'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print({'seed': SEED, 'device': DEVICE})\n",
    "\n",
    "# GPU-oriented math setting (safe no-op on CPU or older torch)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a6e56",
   "metadata": {},
   "source": [
    "## 3) Configuration\n",
    "\n",
    "This section defines local data paths, processed-cache behavior, and training controls for the LANL 2017 network-flow workflow used in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "049a149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and run configuration (CPU/local)\n",
    "DATASET_MODE = 'lanl2017_network'\n",
    "DATA_DIR_OVERRIDE = os.environ.get('P5_DATA_DIR', '').strip()\n",
    "EXTRA_RAW_SEARCH_DIRS = []\n",
    "\n",
    "candidate_data_dirs = []\n",
    "if DATA_DIR_OVERRIDE:\n",
    "    candidate_data_dirs.append(Path(DATA_DIR_OVERRIDE))\n",
    "candidate_data_dirs.extend([\n",
    "    Path('data'),\n",
    "    Path('P5/data'),\n",
    "    Path('../P5/data'),\n",
    "])\n",
    "\n",
    "selected_data_dir = None\n",
    "for cand in candidate_data_dirs:\n",
    "    try:\n",
    "        if cand.exists() and ((cand / 'raw').exists() or (cand / 'processed').exists()):\n",
    "            selected_data_dir = cand\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if selected_data_dir is None:\n",
    "    selected_data_dir = Path('data')\n",
    "\n",
    "DATA_DIR = selected_data_dir\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_BASE_DIR = DATA_DIR / 'processed'\n",
    "\n",
    "PROCESSED_SUBDIR = os.environ.get('P5_PROCESSED_SUBDIR', '').strip()\n",
    "if not PROCESSED_SUBDIR and (PROCESSED_BASE_DIR / '2017').exists():\n",
    "    PROCESSED_SUBDIR = '2017'\n",
    "PROCESSED_DIR = (PROCESSED_BASE_DIR / PROCESSED_SUBDIR) if PROCESSED_SUBDIR else PROCESSED_BASE_DIR\n",
    "MODEL_DIR = PROCESSED_DIR / 'models'\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Raw-first behavior for CPU/local workflow\n",
    "USE_PROCESSED_CACHE_IF_AVAILABLE = False\n",
    "WRITE_PROCESSED_CACHE = True\n",
    "ENFORCE_RAW_MB_PER_SOURCE = True\n",
    "MAX_RAW_MB_PER_SOURCE = 50\n",
    "\n",
    "# Optional raw controls\n",
    "_env_raw_list = os.environ.get('P5_RAW_FILE_LIST', '').strip()\n",
    "RAW_FILE_LIST = [x.strip() for x in _env_raw_list.split(';') if x.strip()]\n",
    "RAW_FILE_OVERRIDE = os.environ.get('P5_RAW_FILE_OVERRIDE', '').strip()\n",
    "AUTO_SELECT_2017_NETFLOW = True\n",
    "AUTO_SELECT_2017_MAX_DAYS = 2\n",
    "\n",
    "# Optional WLS context controls\n",
    "WLS_CONTEXT_FILE_LIST = []\n",
    "AUTO_SELECT_WLS_CONTEXT = True\n",
    "AUTO_SELECT_WLS_MAX_FILES = 2\n",
    "MAX_WLS_CONTEXT_LINES = 50000\n",
    "\n",
    "NETWORK2017_PATH = RAW_DIR / '2017' / 'netflow_day-03.csv'\n",
    "\n",
    "# Parsing/sample controls\n",
    "MAX_EVENTS = 120000\n",
    "MIN_TOK_FREQ = 3\n",
    "MAX_VOCAB = 20000\n",
    "SEQ_LEN = 18\n",
    "STRIDE = 6\n",
    "\n",
    "# EDA controls\n",
    "EDA_ENABLE_PLOTS = True\n",
    "EDA_SAMPLE_MAX = 30000\n",
    "\n",
    "# Tokenization controls\n",
    "TIME_BIN_SECONDS = 300\n",
    "COMP_BUCKETS = 2048\n",
    "TRAIN_FRACTION = 0.90\n",
    "\n",
    "# Model/training controls (CPU-safe defaults)\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "EMBED_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.20\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "D_MODEL = EMBED_DIM\n",
    "N_HEADS = 4\n",
    "N_LAYERS = NUM_LAYERS\n",
    "BLOCK_SIZE = 64\n",
    "\n",
    "GEN_MAX_NEW_TOKENS = 96\n",
    "GEN_TEMPERATURE = 0.9\n",
    "GEN_TOP_K = 20\n",
    "NUM_GENERATIONS = 8\n",
    "\n",
    "RUN_ABLATIONS = True\n",
    "ABLATION_EPOCHS = 2\n",
    "SAVE_MODEL_CHECKPOINTS = True\n",
    "SAVE_ALL_ABLATION_CHECKPOINTS = False\n",
    "\n",
    "DATALOADER_NUM_WORKERS = 0\n",
    "DATALOADER_PIN_MEMORY = False\n",
    "DATALOADER_PERSISTENT_WORKERS = False\n",
    "\n",
    "# Optional runtime budget (None => uncapped)\n",
    "MAX_TRAIN_MINUTES = None\n",
    "TRAINING_BUDGET_MINUTES = MAX_TRAIN_MINUTES\n",
    "\n",
    "PROCESSED_EVENTS_PATH = PROCESSED_DIR / f'p5_{DATASET_MODE}_events_subset.csv.gz'\n",
    "PROCESSED_EVENTS_META_PATH = PROCESSED_DIR / f'p5_{DATASET_MODE}_events_meta.json'\n",
    "PROCESSED_WLS_PATH = PROCESSED_DIR / 'p5_wls_context_subset.csv.gz'\n",
    "TOKENIZER_PATH = PROCESSED_DIR / f'p5_{DATASET_MODE}_tokenizer.pkl'\n",
    "MODEL_BASELINE_PATH = MODEL_DIR / 'baseline_lm.pt'\n",
    "MODEL_ABLATION_PATH = MODEL_DIR / 'ablation_lm.pt'\n",
    "METRICS_PATH = PROCESSED_DIR / 'metrics_summary.csv'\n",
    "GENERATIONS_PATH = PROCESSED_DIR / 'generated_samples.csv'\n",
    "RCA_PATH = PROCESSED_DIR / 'rca_hypotheses.csv'\n",
    "\n",
    "print({'DATA_DIR': str(DATA_DIR), 'RAW_DIR': str(RAW_DIR), 'PROCESSED_DIR': str(PROCESSED_DIR), 'DEVICE': DEVICE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee5585b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content exists: True\n",
      "/content/drive exists: True\n",
      "/content/drive/MyDrive exists: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1647279076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mcache_candidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*events_subset.csv.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mcache_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mrglob\u001b[0;34m(self, pattern, case_sensitive)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mpattern_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"**\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_select_from\u001b[0;34m(self, parent_path, scandir)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0msuccessor_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstarting_point\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterate_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuccessor_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_select_from\u001b[0;34m(self, parent_path, scandir)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# We must close the scandir() object before proceeding to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# avoid exhausting file descriptors when globbing deep trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscandir_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_scandir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;31m# pathlib.Path to customize how the filesystem is accessed. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;31m# includes scandir(), which is used to implement glob().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_child_relpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Disabled for CPU/local workflow. Use cells 8 and 14 only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e87149",
   "metadata": {},
   "source": [
    "## 4) Data Ingestion and Inspection\n",
    "\n",
    "This section loads LANL 2017 network-flow data from local files, displays representative samples, and checks structure. It reads line-by-line so very large files can be processed without loading the entire file into memory.\n",
    "\n",
    "If a processed subset already exists, the notebook can load it directly to avoid repeated heavy ingest work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5417a",
   "metadata": {},
   "source": [
    "### File Integrity Probe\n",
    "\n",
    "This probe checks a small partition of each selected CSV/JSON source before full ingest. It reads only limited lines/chunks to detect corruption or delimiter/format issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48ebf018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probe_netflow_files': [], 'probe_wls_files': []}\n"
     ]
    }
   ],
   "source": [
    "PROBE_NETFLOW_LINES = 2000\n",
    "PROBE_RANDOM_LINES = 8\n",
    "PROBE_WLS_LINES = 2000\n",
    "\n",
    "\n",
    "def _probe_select_netflow_files():\n",
    "    if RAW_FILE_LIST:\n",
    "        return [Path(x) for x in RAW_FILE_LIST if Path(x).exists()]\n",
    "    p2017 = RAW_DIR / '2017'\n",
    "    if p2017.exists():\n",
    "        return sorted(p2017.glob('netflow_day-*.csv'))[:AUTO_SELECT_2017_MAX_DAYS]\n",
    "    return []\n",
    "\n",
    "\n",
    "def _probe_select_wls_files():\n",
    "    if WLS_CONTEXT_FILE_LIST:\n",
    "        return [Path(x) for x in WLS_CONTEXT_FILE_LIST if Path(x).exists()]\n",
    "    p2017 = RAW_DIR / '2017'\n",
    "    if p2017.exists():\n",
    "        return sorted(p2017.glob('wls_day-*.json'))[:AUTO_SELECT_WLS_MAX_FILES]\n",
    "    return []\n",
    "\n",
    "\n",
    "def _reservoir_sample_lines(path: Path, k: int = 8, max_lines: int = 2000):\n",
    "    sample = []\n",
    "    seen = 0\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line:\n",
    "                continue\n",
    "            seen += 1\n",
    "            if len(sample) < k:\n",
    "                sample.append(line)\n",
    "            else:\n",
    "                j = random.randint(1, seen)\n",
    "                if j <= k:\n",
    "                    sample[j - 1] = line\n",
    "            if seen >= max_lines:\n",
    "                break\n",
    "    return sample, seen\n",
    "\n",
    "\n",
    "probe_netflow_files = _probe_select_netflow_files()\n",
    "probe_wls_files = _probe_select_wls_files()\n",
    "\n",
    "print({'probe_netflow_files': [str(p) for p in probe_netflow_files], 'probe_wls_files': [str(p) for p in probe_wls_files]})\n",
    "\n",
    "netflow_probe_rows = []\n",
    "for fp in probe_netflow_files:\n",
    "    file_mb = fp.stat().st_size / (1024 * 1024)\n",
    "    chunk_ok = False\n",
    "    chunk_rows = 0\n",
    "    chunk_cols = None\n",
    "    chunk_err = None\n",
    "\n",
    "    try:\n",
    "        chunk = pd.read_csv(fp, nrows=PROBE_NETFLOW_LINES, header=None, on_bad_lines='skip')\n",
    "        chunk_ok = True\n",
    "        chunk_rows = int(len(chunk))\n",
    "        chunk_cols = int(chunk.shape[1])\n",
    "    except Exception as e:\n",
    "        chunk_err = str(e)\n",
    "\n",
    "    sampled, seen = _reservoir_sample_lines(fp, k=PROBE_RANDOM_LINES, max_lines=PROBE_NETFLOW_LINES)\n",
    "    field_counts = [len(x.split(',')) for x in sampled]\n",
    "\n",
    "    netflow_probe_rows.append({\n",
    "        'file': fp.name,\n",
    "        'size_mb': round(file_mb, 2),\n",
    "        'chunk_ok': chunk_ok,\n",
    "        'chunk_rows': chunk_rows,\n",
    "        'chunk_cols': chunk_cols,\n",
    "        'sample_lines_seen': seen,\n",
    "        'sample_min_fields': int(min(field_counts)) if field_counts else None,\n",
    "        'sample_max_fields': int(max(field_counts)) if field_counts else None,\n",
    "        'chunk_error': chunk_err,\n",
    "    })\n",
    "\n",
    "netflow_probe_df = pd.DataFrame(netflow_probe_rows)\n",
    "if len(netflow_probe_df) > 0:\n",
    "    display(netflow_probe_df)\n",
    "\n",
    "wls_probe_rows = []\n",
    "for fp in probe_wls_files:\n",
    "    file_mb = fp.stat().st_size / (1024 * 1024)\n",
    "    valid = 0\n",
    "    invalid = 0\n",
    "    seen = 0\n",
    "\n",
    "    with open(fp, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line:\n",
    "                continue\n",
    "            seen += 1\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                if isinstance(obj, dict):\n",
    "                    valid += 1\n",
    "                else:\n",
    "                    invalid += 1\n",
    "            except Exception:\n",
    "                invalid += 1\n",
    "            if seen >= PROBE_WLS_LINES:\n",
    "                break\n",
    "\n",
    "    wls_probe_rows.append({\n",
    "        'file': fp.name,\n",
    "        'size_mb': round(file_mb, 2),\n",
    "        'lines_checked': seen,\n",
    "        'json_valid': valid,\n",
    "        'json_invalid': invalid,\n",
    "        'valid_ratio': (valid / seen) if seen > 0 else None,\n",
    "    })\n",
    "\n",
    "wls_probe_df = pd.DataFrame(wls_probe_rows)\n",
    "if len(wls_probe_df) > 0:\n",
    "    display(wls_probe_df)\n",
    "\n",
    "if len(netflow_probe_df) > 0:\n",
    "    failed_chunks = (~netflow_probe_df['chunk_ok']).sum()\n",
    "    print({'netflow_files': len(netflow_probe_df), 'netflow_failed_chunks': int(failed_chunks)})\n",
    "if len(wls_probe_df) > 0:\n",
    "    low_valid = (wls_probe_df['valid_ratio'] < 0.8).sum()\n",
    "    print({'wls_files': len(wls_probe_df), 'wls_low_json_valid_ratio': int(low_valid)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b971740",
   "metadata": {},
   "source": [
    "### Probe Analysis\n",
    "\n",
    "The probe confirms each selected source is readable and structurally consistent before full ingest. If chunk parsing fails, field counts drift heavily, or WLS JSON validity is low, fix the file set before training. Missing values in optional WLS preview fields can be acceptable when specific keys are absent for certain event types, but core parse fields (time/event id) should remain mostly populated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7295198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed event subset cache: /content/drive/MyDrive/data processed 2017/p5_lanl2017_network_events_subset.csv.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-697933258.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mauto_wls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_auto_select_wls_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTO_SELECT_WLS_MAX_FILES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mauto_wls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0msearch_roots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_search_roots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mauto_wls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_discover_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_roots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'wls_day-*.json'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAUTO_SELECT_WLS_MAX_FILES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mWLS_CONTEXT_FILE_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauto_wls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-697933258.py\u001b[0m in \u001b[0;36m_build_search_roots\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmydrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp5_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'**/P5/data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mroots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp5_data\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'raw'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'2017'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp5_data\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp5_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(self, pattern, case_sensitive)\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0mpattern_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern_parts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_sensitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_select_from\u001b[0;34m(self, parent_path, scandir)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0msuccessor_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstarting_point\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterate_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuccessor_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_select_from\u001b[0;34m(self, parent_path, scandir)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# We must close the scandir() object before proceeding to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# avoid exhausting file descriptors when globbing deep trees.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscandir_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m_scandir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;31m# pathlib.Path to customize how the filesystem is accessed. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;31m# includes scandir(), which is used to implement glob().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_child_relpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def _infer_day_from_name(name: str):\n",
    "    m = re.search(r'day[-_]?0*(\\d+)', str(name).lower())\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def _unique_existing_dirs(paths):\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for p in paths:\n",
    "        try:\n",
    "            d = Path(p).expanduser()\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not d.exists() or not d.is_dir():\n",
    "            continue\n",
    "        try:\n",
    "            key = str(d.resolve())\n",
    "        except Exception:\n",
    "            key = str(d)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(d)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def _build_search_roots():\n",
    "    roots = [\n",
    "        RAW_DIR / '2017',\n",
    "        RAW_DIR,\n",
    "        DATA_DIR,\n",
    "        Path.cwd() / 'data' / 'raw' / '2017',\n",
    "        Path.cwd() / 'data' / 'raw',\n",
    "        Path.cwd() / 'data',\n",
    "        Path.cwd() / 'P5' / 'data' / 'raw' / '2017',\n",
    "        Path.cwd() / 'P5' / 'data' / 'raw',\n",
    "        Path.cwd() / 'P5' / 'data',\n",
    "    ]\n",
    "    for extra in EXTRA_RAW_SEARCH_DIRS:\n",
    "        roots.append(Path(extra))\n",
    "    return _unique_existing_dirs(roots)\n",
    "\n",
    "\n",
    "def _discover_files(search_roots, patterns):\n",
    "    found = []\n",
    "    for root in search_roots:\n",
    "        for pat in patterns:\n",
    "            try:\n",
    "                found.extend([p for p in root.rglob(pat) if p.is_file()])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for p in found:\n",
    "        try:\n",
    "            key = str(p.resolve())\n",
    "        except Exception:\n",
    "            key = str(p)\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(p)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def _pick_by_day(files, max_days: int):\n",
    "    if not files:\n",
    "        return []\n",
    "    by_day = {}\n",
    "    unknown = []\n",
    "    for p in files:\n",
    "        day = _infer_day_from_name(p.name)\n",
    "        if day is None:\n",
    "            unknown.append(p)\n",
    "        else:\n",
    "            by_day.setdefault(day, []).append(p)\n",
    "\n",
    "    picked = []\n",
    "    for day in sorted(by_day.keys()):\n",
    "        day_files = sorted(by_day[day], key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "        picked.append(day_files[0])\n",
    "        if len(picked) >= max_days:\n",
    "            return picked\n",
    "\n",
    "    if len(picked) < max_days and unknown:\n",
    "        unknown_sorted = sorted(unknown, key=lambda x: x.stat().st_size if x.exists() else 0, reverse=True)\n",
    "        for p in unknown_sorted:\n",
    "            picked.append(p)\n",
    "            if len(picked) >= max_days:\n",
    "                break\n",
    "\n",
    "    return picked\n",
    "\n",
    "\n",
    "def _auto_select_2017_netflow_files(max_days: int = 2):\n",
    "    net_dir = RAW_DIR / '2017'\n",
    "    if not net_dir.exists():\n",
    "        return []\n",
    "    files = sorted(net_dir.glob('netflow_day-*.csv'))\n",
    "    if not files:\n",
    "        files = sorted(net_dir.glob('netflow_day-*.txt'))\n",
    "    return _pick_by_day(files, max_days=max_days)\n",
    "\n",
    "\n",
    "def _auto_select_wls_files(max_files: int = 2):\n",
    "    wls_dir = RAW_DIR / '2017'\n",
    "    if not wls_dir.exists():\n",
    "        return []\n",
    "    files = sorted(wls_dir.glob('wls_day-*.json'))\n",
    "    return files[:max_files]\n",
    "\n",
    "\n",
    "def ensure_raw_files():\n",
    "    if RAW_FILE_LIST:\n",
    "        selected = []\n",
    "        missing = []\n",
    "        for fp in RAW_FILE_LIST:\n",
    "            p = Path(fp)\n",
    "            if p.exists() and p.is_file():\n",
    "                selected.append(p)\n",
    "            else:\n",
    "                missing.append(fp)\n",
    "        if missing:\n",
    "            raise FileNotFoundError(\n",
    "                f'RAW_FILE_LIST entries not found: {missing}. '\n",
    "                'Use paths like data/raw/2017/netflow_day-03.csv'\n",
    "            )\n",
    "        return selected\n",
    "\n",
    "    if RAW_FILE_OVERRIDE:\n",
    "        p = Path(RAW_FILE_OVERRIDE)\n",
    "        if p.exists() and p.is_file():\n",
    "            return [p]\n",
    "\n",
    "    if AUTO_SELECT_2017_NETFLOW:\n",
    "        auto_files = _auto_select_2017_netflow_files(max_days=AUTO_SELECT_2017_MAX_DAYS)\n",
    "        if auto_files:\n",
    "            print('Auto-selected LANL 2017 netflow files:', [str(p) for p in auto_files])\n",
    "            return auto_files\n",
    "\n",
    "    search_roots = _build_search_roots()\n",
    "    discovered = _discover_files(search_roots, ['netflow_day-*.csv', 'netflow_day-*.txt'])\n",
    "    if discovered:\n",
    "        discovered_sorted = sorted(discovered, key=lambda x: (_infer_day_from_name(x.name) or 9999, str(x)))\n",
    "        selected = _pick_by_day(discovered_sorted, max_days=AUTO_SELECT_2017_MAX_DAYS)\n",
    "        print('Discovered LANL netflow files:', [str(p) for p in selected])\n",
    "        return selected\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        'No local netflow files found. '\n",
    "        'Place LANL files under data/raw/2017/ (for example data/raw/2017/netflow_day-03.csv) '\n",
    "        'or set P5_DATA_DIR / P5_RAW_FILE_LIST / P5_RAW_FILE_OVERRIDE.'\n",
    "    )\n",
    "\n",
    "\n",
    "def _iter_lines(path: Path):\n",
    "    if path.suffix == '.gz':\n",
    "        with gzip.open(path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                yield line.rstrip('\\n')\n",
    "    else:\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                yield line.rstrip('\\n')\n",
    "\n",
    "\n",
    "def _to_int(x):\n",
    "    try:\n",
    "        return int(float(str(x).strip()))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _to_float(x):\n",
    "    try:\n",
    "        return float(str(x).strip())\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_lanl2017_network(line: str):\n",
    "    parts = [p.strip() for p in line.split(',')]\n",
    "    if len(parts) < 11:\n",
    "        return None\n",
    "\n",
    "    t, dur, src_dev, dst_dev, proto, src_port, dst_port, src_pkts, dst_pkts, src_bytes, dst_bytes = parts[:11]\n",
    "    t_int = _to_int(t)\n",
    "    if t_int is None:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        'time': t_int,\n",
    "        'src_user': 'UNK',\n",
    "        'dst_user': 'UNK',\n",
    "        'src_comp': src_dev,\n",
    "        'dst_comp': dst_dev,\n",
    "        'auth_type': f'PROTO_{proto}',\n",
    "        'logon_type': f'SPORT_{src_port}',\n",
    "        'orientation': 'NETFLOW',\n",
    "        'status': 'FLOW',\n",
    "        'duration': _to_float(dur),\n",
    "        'protocol': str(proto),\n",
    "        'src_port': src_port,\n",
    "        'dst_port': dst_port,\n",
    "        'src_packets': _to_int(src_pkts),\n",
    "        'dst_packets': _to_int(dst_pkts),\n",
    "        'src_bytes': _to_int(src_bytes),\n",
    "        'dst_bytes': _to_int(dst_bytes),\n",
    "    }\n",
    "\n",
    "\n",
    "raw_load_mode = 'processed_cache' if (USE_PROCESSED_CACHE_IF_AVAILABLE and PROCESSED_EVENTS_PATH.exists()) else 'raw_stream'\n",
    "raw_paths = []\n",
    "\n",
    "if raw_load_mode == 'processed_cache':\n",
    "    raw_df = pd.read_csv(PROCESSED_EVENTS_PATH)\n",
    "    print('Loaded processed event subset cache:', PROCESSED_EVENTS_PATH)\n",
    "else:\n",
    "    raw_paths = ensure_raw_files()\n",
    "    print('Using raw files:', [str(p) for p in raw_paths])\n",
    "\n",
    "if not WLS_CONTEXT_FILE_LIST and AUTO_SELECT_WLS_CONTEXT:\n",
    "    auto_wls = _auto_select_wls_files(max_files=AUTO_SELECT_WLS_MAX_FILES)\n",
    "    WLS_CONTEXT_FILE_LIST = [str(p) for p in auto_wls]\n",
    "    if WLS_CONTEXT_FILE_LIST:\n",
    "        print('Auto-selected WLS context files:', WLS_CONTEXT_FILE_LIST)\n",
    "\n",
    "if raw_load_mode != 'processed_cache':\n",
    "    byte_cap = int(MAX_RAW_MB_PER_SOURCE * 1024 * 1024)\n",
    "    source_bytes = {}\n",
    "    records = []\n",
    "\n",
    "    for raw_file in raw_paths:\n",
    "        day_num = _infer_day_from_name(raw_file.name)\n",
    "        source_bytes.setdefault(raw_file.name, 0)\n",
    "\n",
    "        for line in _iter_lines(raw_file):\n",
    "            if ENFORCE_RAW_MB_PER_SOURCE and source_bytes[raw_file.name] >= byte_cap:\n",
    "                break\n",
    "            source_bytes[raw_file.name] += len(line.encode('utf-8', errors='ignore')) + 1\n",
    "\n",
    "            obj = parse_lanl2017_network(line)\n",
    "            if obj is not None:\n",
    "                obj['source_file'] = raw_file.name\n",
    "                obj['data_day'] = day_num\n",
    "                records.append(obj)\n",
    "\n",
    "            if MAX_EVENTS is not None and len(records) >= MAX_EVENTS:\n",
    "                break\n",
    "\n",
    "        if MAX_EVENTS is not None and len(records) >= MAX_EVENTS:\n",
    "            break\n",
    "\n",
    "    if len(records) < 2000:\n",
    "        raise ValueError('Too few parsed events. Verify netflow file format.')\n",
    "\n",
    "    raw_df = pd.DataFrame(records)\n",
    "\n",
    "    if WRITE_PROCESSED_CACHE:\n",
    "        raw_df.to_csv(PROCESSED_EVENTS_PATH, index=False, compression='gzip')\n",
    "        meta = {\n",
    "            'dataset_mode': DATASET_MODE,\n",
    "            'source_files': [str(p) for p in raw_paths],\n",
    "            'events': int(len(raw_df)),\n",
    "            'max_events': int(MAX_EVENTS) if MAX_EVENTS is not None else None,\n",
    "            'enforce_raw_mb_per_source': bool(ENFORCE_RAW_MB_PER_SOURCE),\n",
    "            'max_raw_mb_per_source': float(MAX_RAW_MB_PER_SOURCE),\n",
    "            'raw_bytes_read_per_source': source_bytes,\n",
    "            'created_utc': datetime.now(timezone.utc).isoformat(),\n",
    "        }\n",
    "        with open(PROCESSED_EVENTS_META_PATH, 'w', encoding='utf-8') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        print('Wrote processed event subset cache:', PROCESSED_EVENTS_PATH)\n",
    "        print('Wrote processed event metadata:', PROCESSED_EVENTS_META_PATH)\n",
    "\n",
    "print('raw_load_mode:', raw_load_mode)\n",
    "print('Loaded events:', len(raw_df))\n",
    "print('Loaded source files:', raw_df['source_file'].nunique())\n",
    "if 'data_day' in raw_df.columns:\n",
    "    print('Loaded days:', sorted([d for d in raw_df['data_day'].dropna().unique().tolist()]))\n",
    "\n",
    "display(raw_df.head(10))\n",
    "display(raw_df.describe(include='all').head(12))\n",
    "\n",
    "\n",
    "def _extract_wls_fields(line: str):\n",
    "    try:\n",
    "        obj = json.loads(line)\n",
    "        if isinstance(obj, dict):\n",
    "            return {\n",
    "                'timestamp': obj.get('Time') or obj.get('time') or obj.get('TimeCreated') or obj.get('timestamp'),\n",
    "                'event_id': obj.get('EventID') or obj.get('event_id') or obj.get('id'),\n",
    "                'computer': obj.get('LogHost') or obj.get('Computer') or obj.get('computer') or obj.get('host'),\n",
    "                'channel': obj.get('LogonTypeDescription') or obj.get('LogonType') or obj.get('Channel') or obj.get('channel'),\n",
    "                'provider': obj.get('AuthenticationPackage') or obj.get('ParentProcessName') or obj.get('ProviderName') or obj.get('provider') or obj.get('Provider'),\n",
    "                'user': obj.get('UserName') or obj.get('user'),\n",
    "                'domain': obj.get('DomainName') or obj.get('domain'),\n",
    "                'source': obj.get('Source') or obj.get('source'),\n",
    "                'raw_line': line[:500],\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\n",
    "        'timestamp': None,\n",
    "        'event_id': None,\n",
    "        'computer': None,\n",
    "        'channel': None,\n",
    "        'provider': None,\n",
    "        'user': None,\n",
    "        'domain': None,\n",
    "        'source': None,\n",
    "        'raw_line': line[:500],\n",
    "    }\n",
    "\n",
    "\n",
    "wls_context_df = None\n",
    "if WLS_CONTEXT_FILE_LIST:\n",
    "    preview_rows = []\n",
    "    lines_seen = 0\n",
    "    for fp in WLS_CONTEXT_FILE_LIST:\n",
    "        p = Path(fp)\n",
    "        if not p.exists():\n",
    "            print('WLS context file not found:', p)\n",
    "            continue\n",
    "        for line in _iter_lines(p):\n",
    "            row = _extract_wls_fields(line)\n",
    "            row['source_file'] = p.name\n",
    "            preview_rows.append(row)\n",
    "            lines_seen += 1\n",
    "            if lines_seen >= MAX_WLS_CONTEXT_LINES:\n",
    "                break\n",
    "        if lines_seen >= MAX_WLS_CONTEXT_LINES:\n",
    "            break\n",
    "\n",
    "    if preview_rows:\n",
    "        wls_context_df = pd.DataFrame(preview_rows)\n",
    "        print('Loaded WLS context lines:', len(wls_context_df))\n",
    "        display(wls_context_df.head(10))\n",
    "\n",
    "        if WRITE_PROCESSED_CACHE:\n",
    "            wls_context_df.to_csv(PROCESSED_WLS_PATH, index=False, compression='gzip')\n",
    "            print('Wrote processed WLS context subset:', PROCESSED_WLS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4af8ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed event subset cache: /content/drive/MyDrive/data processed 2017/p5_lanl2017_network_events_subset.csv.gz\n",
      "Rows: 300000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6dff4351-e8f7-442d-aa89-75fa721a141d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>src_user</th>\n",
       "      <th>dst_user</th>\n",
       "      <th>src_comp</th>\n",
       "      <th>dst_comp</th>\n",
       "      <th>auth_type</th>\n",
       "      <th>logon_type</th>\n",
       "      <th>orientation</th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol</th>\n",
       "      <th>src_port</th>\n",
       "      <th>dst_port</th>\n",
       "      <th>src_packets</th>\n",
       "      <th>dst_packets</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>source_file</th>\n",
       "      <th>data_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118781</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Comp364445</td>\n",
       "      <td>Comp547245</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_Port05507</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>5580.0</td>\n",
       "      <td>17</td>\n",
       "      <td>Port05507</td>\n",
       "      <td>Port46272</td>\n",
       "      <td>0</td>\n",
       "      <td>755065</td>\n",
       "      <td>0</td>\n",
       "      <td>1042329018</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118783</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Comp450942</td>\n",
       "      <td>Comp829338</td>\n",
       "      <td>PROTO_6</td>\n",
       "      <td>SPORT_Port03137</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>6976.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Port03137</td>\n",
       "      <td>445</td>\n",
       "      <td>1665</td>\n",
       "      <td>1108</td>\n",
       "      <td>300810</td>\n",
       "      <td>250408</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118785</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>IP564116</td>\n",
       "      <td>Comp141988</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_5060</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>14178.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5060</td>\n",
       "      <td>5060</td>\n",
       "      <td>1866</td>\n",
       "      <td>0</td>\n",
       "      <td>1477041</td>\n",
       "      <td>0</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118785</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>IP564116</td>\n",
       "      <td>Comp141988</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_5060</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>28147.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5060</td>\n",
       "      <td>5060</td>\n",
       "      <td>3326</td>\n",
       "      <td>0</td>\n",
       "      <td>2656305</td>\n",
       "      <td>0</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118785</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>IP564116</td>\n",
       "      <td>Comp141988</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_5060</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>48507.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5060</td>\n",
       "      <td>5060</td>\n",
       "      <td>5423</td>\n",
       "      <td>0</td>\n",
       "      <td>4388449</td>\n",
       "      <td>0</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dff4351-e8f7-442d-aa89-75fa721a141d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6dff4351-e8f7-442d-aa89-75fa721a141d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6dff4351-e8f7-442d-aa89-75fa721a141d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     time src_user dst_user    src_comp    dst_comp auth_type  \\\n",
       "0  118781      UNK      UNK  Comp364445  Comp547245  PROTO_17   \n",
       "1  118783      UNK      UNK  Comp450942  Comp829338   PROTO_6   \n",
       "2  118785      UNK      UNK    IP564116  Comp141988  PROTO_17   \n",
       "3  118785      UNK      UNK    IP564116  Comp141988  PROTO_17   \n",
       "4  118785      UNK      UNK    IP564116  Comp141988  PROTO_17   \n",
       "\n",
       "        logon_type orientation status  duration  protocol   src_port  \\\n",
       "0  SPORT_Port05507     NETFLOW   FLOW    5580.0        17  Port05507   \n",
       "1  SPORT_Port03137     NETFLOW   FLOW    6976.0         6  Port03137   \n",
       "2       SPORT_5060     NETFLOW   FLOW   14178.0        17       5060   \n",
       "3       SPORT_5060     NETFLOW   FLOW   28147.0        17       5060   \n",
       "4       SPORT_5060     NETFLOW   FLOW   48507.0        17       5060   \n",
       "\n",
       "    dst_port  src_packets  dst_packets  src_bytes   dst_bytes  \\\n",
       "0  Port46272            0       755065          0  1042329018   \n",
       "1        445         1665         1108     300810      250408   \n",
       "2       5060         1866            0    1477041           0   \n",
       "3       5060         3326            0    2656305           0   \n",
       "4       5060         5423            0    4388449           0   \n",
       "\n",
       "          source_file  data_day  \n",
       "0  netflow_day-02.csv         2  \n",
       "1  netflow_day-02.csv         2  \n",
       "2  netflow_day-02.csv         2  \n",
       "3  netflow_day-02.csv         2  \n",
       "4  netflow_day-02.csv         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disabled for CPU/local workflow. Use cells 8 and 14 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Disabled for CPU/local workflow. Use cells 8 and 14 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef30a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_load_mode: processed_cache\n",
      "Loaded events: 300000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8b389937-3be2-46a2-ac54-ce762fbcd469\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>src_user</th>\n",
       "      <th>dst_user</th>\n",
       "      <th>src_comp</th>\n",
       "      <th>dst_comp</th>\n",
       "      <th>auth_type</th>\n",
       "      <th>logon_type</th>\n",
       "      <th>orientation</th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol</th>\n",
       "      <th>src_port</th>\n",
       "      <th>dst_port</th>\n",
       "      <th>src_packets</th>\n",
       "      <th>dst_packets</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>source_file</th>\n",
       "      <th>data_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118781</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Comp364445</td>\n",
       "      <td>Comp547245</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_Port05507</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>5580.0</td>\n",
       "      <td>17</td>\n",
       "      <td>Port05507</td>\n",
       "      <td>Port46272</td>\n",
       "      <td>0</td>\n",
       "      <td>755065</td>\n",
       "      <td>0</td>\n",
       "      <td>1042329018</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118783</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Comp450942</td>\n",
       "      <td>Comp829338</td>\n",
       "      <td>PROTO_6</td>\n",
       "      <td>SPORT_Port03137</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>6976.0</td>\n",
       "      <td>6</td>\n",
       "      <td>Port03137</td>\n",
       "      <td>445</td>\n",
       "      <td>1665</td>\n",
       "      <td>1108</td>\n",
       "      <td>300810</td>\n",
       "      <td>250408</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118785</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>IP564116</td>\n",
       "      <td>Comp141988</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_5060</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>14178.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5060</td>\n",
       "      <td>5060</td>\n",
       "      <td>1866</td>\n",
       "      <td>0</td>\n",
       "      <td>1477041</td>\n",
       "      <td>0</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118785</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>IP564116</td>\n",
       "      <td>Comp141988</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_5060</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>28147.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5060</td>\n",
       "      <td>5060</td>\n",
       "      <td>3326</td>\n",
       "      <td>0</td>\n",
       "      <td>2656305</td>\n",
       "      <td>0</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118785</td>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>IP564116</td>\n",
       "      <td>Comp141988</td>\n",
       "      <td>PROTO_17</td>\n",
       "      <td>SPORT_5060</td>\n",
       "      <td>NETFLOW</td>\n",
       "      <td>FLOW</td>\n",
       "      <td>48507.0</td>\n",
       "      <td>17</td>\n",
       "      <td>5060</td>\n",
       "      <td>5060</td>\n",
       "      <td>5423</td>\n",
       "      <td>0</td>\n",
       "      <td>4388449</td>\n",
       "      <td>0</td>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b389937-3be2-46a2-ac54-ce762fbcd469')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8b389937-3be2-46a2-ac54-ce762fbcd469 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8b389937-3be2-46a2-ac54-ce762fbcd469');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     time src_user dst_user    src_comp    dst_comp auth_type  \\\n",
       "0  118781      UNK      UNK  Comp364445  Comp547245  PROTO_17   \n",
       "1  118783      UNK      UNK  Comp450942  Comp829338   PROTO_6   \n",
       "2  118785      UNK      UNK    IP564116  Comp141988  PROTO_17   \n",
       "3  118785      UNK      UNK    IP564116  Comp141988  PROTO_17   \n",
       "4  118785      UNK      UNK    IP564116  Comp141988  PROTO_17   \n",
       "\n",
       "        logon_type orientation status  duration  protocol   src_port  \\\n",
       "0  SPORT_Port05507     NETFLOW   FLOW    5580.0        17  Port05507   \n",
       "1  SPORT_Port03137     NETFLOW   FLOW    6976.0         6  Port03137   \n",
       "2       SPORT_5060     NETFLOW   FLOW   14178.0        17       5060   \n",
       "3       SPORT_5060     NETFLOW   FLOW   28147.0        17       5060   \n",
       "4       SPORT_5060     NETFLOW   FLOW   48507.0        17       5060   \n",
       "\n",
       "    dst_port  src_packets  dst_packets  src_bytes   dst_bytes  \\\n",
       "0  Port46272            0       755065          0  1042329018   \n",
       "1        445         1665         1108     300810      250408   \n",
       "2       5060         1866            0    1477041           0   \n",
       "3       5060         3326            0    2656305           0   \n",
       "4       5060         5423            0    4388449           0   \n",
       "\n",
       "          source_file  data_day  \n",
       "0  netflow_day-02.csv         2  \n",
       "1  netflow_day-02.csv         2  \n",
       "2  netflow_day-02.csv         2  \n",
       "3  netflow_day-02.csv         2  \n",
       "4  netflow_day-02.csv         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disabled for CPU/local workflow. Use cells 8 and 14 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2eb2a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "gpu: Tesla T4\n",
      "Overrides set: MAX_EVENTS, BATCH_SIZE, EPOCHS, MAX_TRAIN_MINUTES\n"
     ]
    }
   ],
   "source": [
    "# Disabled for CPU/local workflow. Use cells 8 and 14 only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e628ef1",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The ingestion output now reports whether data came from raw stream files or a processed cache, so reruns are transparent and fast. Per-source size caps create deterministic, shareable subsets while preserving real LANL structure. This supports reproducibility and reviewer portability without shipping multi-GB raw files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84448d",
   "metadata": {},
   "source": [
    "## 5) Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before model training, this section quantifies source balance, temporal coverage, and numeric distribution shape. These checks help justify token design, generation settings, and ablation priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "164426c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows': 300000, 'columns': ['time', 'src_user', 'dst_user', 'src_comp', 'dst_comp', 'auth_type', 'logon_type', 'orientation', 'status', 'duration', 'protocol', 'src_port']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3eeefa4c-ee53-4f38-bb6f-cd07d109c8e3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>netflow_day-02.csv</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3eeefa4c-ee53-4f38-bb6f-cd07d109c8e3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3eeefa4c-ee53-4f38-bb6f-cd07d109c8e3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3eeefa4c-ee53-4f38-bb6f-cd07d109c8e3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          source_file    rows\n",
       "0  netflow_day-02.csv  300000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cf8f1c8f-7407-4e81-bae3-74327168b907\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_day</th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf8f1c8f-7407-4e81-bae3-74327168b907')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-cf8f1c8f-7407-4e81-bae3-74327168b907 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-cf8f1c8f-7407-4e81-bae3-74327168b907');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   data_day    rows\n",
       "0         2  300000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a3b5122b-d4d6-40a7-9749-a1e49e19b200\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>nan_count</th>\n",
       "      <th>nan_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>duration</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>src_packets</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dst_packets</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>src_bytes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dst_bytes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3b5122b-d4d6-40a7-9749-a1e49e19b200')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a3b5122b-d4d6-40a7-9749-a1e49e19b200 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a3b5122b-d4d6-40a7-9749-a1e49e19b200');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       feature  nan_count  nan_pct\n",
       "0     duration          0      0.0\n",
       "1  src_packets          0      0.0\n",
       "2  dst_packets          0      0.0\n",
       "3    src_bytes          0      0.0\n",
       "4    dst_bytes          0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-a2586f3a-bb7c-4c91-acbd-f33eb1700ce0\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>1.379577e+06</td>\n",
       "      <td>1.792855e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100600.0</td>\n",
       "      <td>635432.0</td>\n",
       "      <td>1908822.50</td>\n",
       "      <td>7.655151e+06</td>\n",
       "      <td>3.214329e+12</td>\n",
       "      <td>1.6768</td>\n",
       "      <td>1.9940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_packets</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>7.867200e+05</td>\n",
       "      <td>3.036942e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2665.0</td>\n",
       "      <td>20018.0</td>\n",
       "      <td>66885.25</td>\n",
       "      <td>2.396174e+09</td>\n",
       "      <td>9.223015e+14</td>\n",
       "      <td>53.6403</td>\n",
       "      <td>3149.5173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_packets</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>7.478724e+05</td>\n",
       "      <td>3.601702e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>414.00</td>\n",
       "      <td>3.201805e+09</td>\n",
       "      <td>1.297226e+15</td>\n",
       "      <td>63.2702</td>\n",
       "      <td>4400.4956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_bytes</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>1.964466e+08</td>\n",
       "      <td>5.689635e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413364.0</td>\n",
       "      <td>4962866.0</td>\n",
       "      <td>39867216.00</td>\n",
       "      <td>4.193353e+11</td>\n",
       "      <td>3.237195e+19</td>\n",
       "      <td>50.6767</td>\n",
       "      <td>2799.9846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_bytes</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>1.627924e+08</td>\n",
       "      <td>6.521489e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25806.00</td>\n",
       "      <td>5.434128e+11</td>\n",
       "      <td>4.252982e+19</td>\n",
       "      <td>55.7071</td>\n",
       "      <td>3511.0333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2586f3a-bb7c-4c91-acbd-f33eb1700ce0')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a2586f3a-bb7c-4c91-acbd-f33eb1700ce0 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a2586f3a-bb7c-4c91-acbd-f33eb1700ce0');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                count          mean           std  min       25%        50%  \\\n",
       "duration     300000.0  1.379577e+06  1.792855e+06  0.0  100600.0   635432.0   \n",
       "src_packets  300000.0  7.867200e+05  3.036942e+07  0.0    2665.0    20018.0   \n",
       "dst_packets  300000.0  7.478724e+05  3.601702e+07  0.0       0.0        0.0   \n",
       "src_bytes    300000.0  1.964466e+08  5.689635e+09  0.0  413364.0  4962866.0   \n",
       "dst_bytes    300000.0  1.627924e+08  6.521489e+09  0.0       0.0        0.0   \n",
       "\n",
       "                     75%           max      variance  skewness   kurtosis  \n",
       "duration      1908822.50  7.655151e+06  3.214329e+12    1.6768     1.9940  \n",
       "src_packets     66885.25  2.396174e+09  9.223015e+14   53.6403  3149.5173  \n",
       "dst_packets       414.00  3.201805e+09  1.297226e+15   63.2702  4400.4956  \n",
       "src_bytes    39867216.00  4.193353e+11  3.237195e+19   50.6767  2799.9846  \n",
       "dst_bytes       25806.00  5.434128e+11  4.252982e+19   55.7071  3511.0333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5f7e3732-3e8f-4111-b88f-bc901a8ff5ba\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_packets</th>\n",
       "      <th>dst_packets</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>3.214329e+12</td>\n",
       "      <td>-5.934940e+10</td>\n",
       "      <td>-2.853612e+11</td>\n",
       "      <td>1.131009e+14</td>\n",
       "      <td>-7.847991e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_packets</th>\n",
       "      <td>-5.934940e+10</td>\n",
       "      <td>9.223015e+14</td>\n",
       "      <td>1.015642e+15</td>\n",
       "      <td>1.707465e+17</td>\n",
       "      <td>1.705096e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_packets</th>\n",
       "      <td>-2.853612e+11</td>\n",
       "      <td>1.015642e+15</td>\n",
       "      <td>1.297226e+15</td>\n",
       "      <td>1.808065e+17</td>\n",
       "      <td>2.200182e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_bytes</th>\n",
       "      <td>1.131009e+14</td>\n",
       "      <td>1.707465e+17</td>\n",
       "      <td>1.808065e+17</td>\n",
       "      <td>3.237195e+19</td>\n",
       "      <td>2.999110e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dst_bytes</th>\n",
       "      <td>-7.847991e+13</td>\n",
       "      <td>1.705096e+17</td>\n",
       "      <td>2.200182e+17</td>\n",
       "      <td>2.999110e+19</td>\n",
       "      <td>4.252982e+19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f7e3732-3e8f-4111-b88f-bc901a8ff5ba')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5f7e3732-3e8f-4111-b88f-bc901a8ff5ba button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5f7e3732-3e8f-4111-b88f-bc901a8ff5ba');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 duration   src_packets   dst_packets     src_bytes  \\\n",
       "duration     3.214329e+12 -5.934940e+10 -2.853612e+11  1.131009e+14   \n",
       "src_packets -5.934940e+10  9.223015e+14  1.015642e+15  1.707465e+17   \n",
       "dst_packets -2.853612e+11  1.015642e+15  1.297226e+15  1.808065e+17   \n",
       "src_bytes    1.131009e+14  1.707465e+17  1.808065e+17  3.237195e+19   \n",
       "dst_bytes   -7.847991e+13  1.705096e+17  2.200182e+17  2.999110e+19   \n",
       "\n",
       "                dst_bytes  \n",
       "duration    -7.847991e+13  \n",
       "src_packets  1.705096e+17  \n",
       "dst_packets  2.200182e+17  \n",
       "src_bytes    2.999110e+19  \n",
       "dst_bytes    4.252982e+19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'EDA_SAMPLE_MAX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1274716318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msample_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEDA_SAMPLE_MAX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meda_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mplot_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meda_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msample_n\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meda_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0meda_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EDA_SAMPLE_MAX' is not defined"
     ]
    }
   ],
   "source": [
    "eda_df = raw_df.copy()\n",
    "\n",
    "print({'rows': len(eda_df), 'columns': list(eda_df.columns)[:12]})\n",
    "\n",
    "if 'source_file' in eda_df.columns:\n",
    "    src_counts = eda_df['source_file'].value_counts().reset_index()\n",
    "    src_counts.columns = ['source_file', 'rows']\n",
    "    display(src_counts)\n",
    "\n",
    "if 'data_day' in eda_df.columns:\n",
    "    day_counts = eda_df['data_day'].value_counts(dropna=False).sort_index().reset_index()\n",
    "    day_counts.columns = ['data_day', 'rows']\n",
    "    display(day_counts)\n",
    "\n",
    "numeric_cols = [c for c in ['duration', 'src_packets', 'dst_packets', 'src_bytes', 'dst_bytes'] if c in eda_df.columns]\n",
    "if numeric_cols:\n",
    "    nan_summary = eda_df[numeric_cols].isna().sum().reset_index()\n",
    "    nan_summary.columns = ['feature', 'nan_count']\n",
    "    nan_summary['nan_pct'] = 100.0 * nan_summary['nan_count'] / max(len(eda_df), 1)\n",
    "    nan_summary['nan_pct'] = nan_summary['nan_pct'].round(4)\n",
    "    display(nan_summary)\n",
    "\n",
    "    num_stats = eda_df[numeric_cols].describe().T\n",
    "    num_stats['variance'] = eda_df[numeric_cols].var(numeric_only=True)\n",
    "    num_stats['skewness'] = eda_df[numeric_cols].skew(numeric_only=True)\n",
    "    num_stats['kurtosis'] = eda_df[numeric_cols].kurt(numeric_only=True)\n",
    "    display(num_stats.round(4))\n",
    "\n",
    "    cov_df = eda_df[numeric_cols].cov()\n",
    "    display(cov_df.round(4))\n",
    "\n",
    "    sample_n = min(EDA_SAMPLE_MAX, len(eda_df))\n",
    "    plot_df = eda_df[numeric_cols].sample(n=sample_n, random_state=SEED) if sample_n < len(eda_df) else eda_df[numeric_cols]\n",
    "\n",
    "    if EDA_ENABLE_PLOTS:\n",
    "        fig, axes = plt.subplots(2, max(1, len(numeric_cols)), figsize=(4 * max(1, len(numeric_cols)), 7))\n",
    "        axes = np.array(axes).reshape(2, -1)\n",
    "\n",
    "        for j, col in enumerate(numeric_cols):\n",
    "            sns.histplot(plot_df[col].dropna(), bins=50, ax=axes[0, j], color='#4C72B0')\n",
    "            axes[0, j].set_title(f'{col} Histogram')\n",
    "            axes[0, j].set_xlabel(col)\n",
    "            axes[0, j].set_ylabel('Count')\n",
    "\n",
    "            sns.boxplot(x=np.log1p(plot_df[col].dropna()), ax=axes[1, j], color='#55A868')\n",
    "            axes[1, j].set_title(f'log1p({col}) Boxplot')\n",
    "            axes[1, j].set_xlabel(f'log1p({col})')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Scale covariance for readable annotations (fixed decimals, shorter numbers)\n",
    "        cov_abs_max = float(np.nanmax(np.abs(cov_df.to_numpy()))) if cov_df.size > 0 else 0.0\n",
    "        if np.isfinite(cov_abs_max) and cov_abs_max > 0:\n",
    "            cov_exp = int(np.floor(np.log10(cov_abs_max)))\n",
    "            cov_exp3 = int(np.floor(cov_exp / 3.0) * 3)\n",
    "            cov_scale = 10.0 ** cov_exp3\n",
    "        else:\n",
    "            cov_exp3 = 0\n",
    "            cov_scale = 1.0\n",
    "\n",
    "        cov_plot = cov_df / cov_scale\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cov_plot, annot=True, fmt='.2f', cmap='Blues', annot_kws={'size': 9})\n",
    "        if cov_exp3 != 0:\n",
    "            plt.title(f'Covariance Matrix (Numeric Event Fields, scaled by 1e{cov_exp3})')\n",
    "        else:\n",
    "            plt.title('Covariance Matrix (Numeric Event Fields)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "if 'protocol' in eda_df.columns:\n",
    "    top_proto = eda_df['protocol'].astype(str).value_counts().head(12).reset_index()\n",
    "    top_proto.columns = ['protocol', 'count']\n",
    "    display(top_proto)\n",
    "\n",
    "    if EDA_ENABLE_PLOTS:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.barplot(data=top_proto, x='protocol', y='count', color='#C44E52')\n",
    "        plt.title('Top Protocol Values')\n",
    "        plt.xlabel('Protocol')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1ed3b",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "This EDA quantifies whether the subset preserves meaningful structure from the raw stream. Numeric moments (mean, variance, skewness, kurtosis), covariance, and protocol concentration provide evidence about heavy tails and imbalance, which informs tokenization and interpretation of generated sequences. Any NaNs in the numeric summary are expected only when fields are missing or non-parsable in sampled lines; persistent high NaN percentages in core numeric fields would indicate ingest quality issues that should be fixed before training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55dd82a",
   "metadata": {},
   "source": [
    "## 6) Preprocessing and Sequence Construction\n",
    "\n",
    "Events are transformed into compact symbolic tokens for sequence modeling. This section now enforces a modeling row cap (`MAX_EVENTS`) even when a larger processed cache is loaded, so quick and bounded runs stay consistent with runtime presets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "936e8ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318804bed4e24c3d807a1c8a09813c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing events:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'TIME_BIN_SECONDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4029663054.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mrows_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tokenizing events'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mall_token_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mevent_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0mflat_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_token_sequences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4029663054.py\u001b[0m in \u001b[0;36mevent_to_tokens\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_row_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtbin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mTIME_BIN_SECONDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstable_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_row_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'src_comp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'UNK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOMP_BUCKETS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TIME_BIN_SECONDS' is not defined"
     ]
    }
   ],
   "source": [
    "def _row_get(row, key, default='UNK'):\n",
    "    # supports dict-like rows and namedtuples from itertuples\n",
    "    if isinstance(row, dict):\n",
    "        return row.get(key, default)\n",
    "    return getattr(row, key, default)\n",
    "\n",
    "\n",
    "def stable_bucket(value: str, n_buckets: int) -> int:\n",
    "    if value is None:\n",
    "        return 0\n",
    "    s = str(value)\n",
    "    h = hashlib.md5(s.encode('utf-8')).hexdigest()\n",
    "    return int(h, 16) % n_buckets\n",
    "\n",
    "\n",
    "def clean_token(s: str, default='UNK'):\n",
    "    if s is None:\n",
    "        return default\n",
    "    s = str(s).strip()\n",
    "    if s in {'', '?', 'nan', 'None'}:\n",
    "        return default\n",
    "    s = re.sub(r'[^A-Za-z0-9_-]', '_', s)\n",
    "    return s[:40]\n",
    "\n",
    "\n",
    "def magnitude_bin(v):\n",
    "    try:\n",
    "        x = max(float(v), 0.0)\n",
    "        return int(np.log1p(x))\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def event_to_tokens(row):\n",
    "    t = _row_get(row, 'time', 0)\n",
    "    t = t if pd.notna(t) else 0\n",
    "    tbin = int(t) // TIME_BIN_SECONDS\n",
    "\n",
    "    sc = stable_bucket(clean_token(_row_get(row, 'src_comp', 'UNK')), COMP_BUCKETS)\n",
    "    dc = stable_bucket(clean_token(_row_get(row, 'dst_comp', 'UNK')), COMP_BUCKETS)\n",
    "\n",
    "    proto = clean_token(_row_get(row, 'protocol', 'UNK'))\n",
    "    sport = clean_token(_row_get(row, 'src_port', 'UNK'))\n",
    "    dport = clean_token(_row_get(row, 'dst_port', 'UNK'))\n",
    "\n",
    "    dur_b = magnitude_bin(_row_get(row, 'duration', 0))\n",
    "    spk_b = magnitude_bin(_row_get(row, 'src_packets', 0))\n",
    "    dpk_b = magnitude_bin(_row_get(row, 'dst_packets', 0))\n",
    "    sby_b = magnitude_bin(_row_get(row, 'src_bytes', 0))\n",
    "    dby_b = magnitude_bin(_row_get(row, 'dst_bytes', 0))\n",
    "\n",
    "    return [\n",
    "        f'TB_{tbin}',\n",
    "        f'SC_{sc}',\n",
    "        f'DC_{dc}',\n",
    "        f'PR_{proto}',\n",
    "        f'SP_{sport}',\n",
    "        f'DP_{dport}',\n",
    "        f'DU_{dur_b}',\n",
    "        f'SPK_{spk_b}',\n",
    "        f'DPK_{dpk_b}',\n",
    "        f'SBY_{sby_b}',\n",
    "        f'DBY_{dby_b}',\n",
    "        '<EOS>',\n",
    "    ]\n",
    "\n",
    "\n",
    "model_df = raw_df.copy()\n",
    "if MAX_EVENTS is not None and len(model_df) > int(MAX_EVENTS):\n",
    "    if 'time' in model_df.columns:\n",
    "        model_df = model_df.sort_values('time', kind='stable').head(int(MAX_EVENTS)).reset_index(drop=True)\n",
    "    else:\n",
    "        model_df = model_df.head(int(MAX_EVENTS)).reset_index(drop=True)\n",
    "\n",
    "if len(model_df) < 2000:\n",
    "    raise ValueError('Too few events for sequence modeling after cap/filter. Increase MAX_EVENTS.')\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm as _tqdm\n",
    "except Exception:\n",
    "    _tqdm = None\n",
    "\n",
    "rows_iter = model_df.itertuples(index=False, name='EventRow')\n",
    "if _tqdm is not None:\n",
    "    rows_iter = _tqdm(rows_iter, total=len(model_df), desc='Tokenizing events', leave=False)\n",
    "\n",
    "all_token_sequences = [event_to_tokens(row) for row in rows_iter]\n",
    "flat_tokens = [tok for seq in all_token_sequences for tok in seq]\n",
    "\n",
    "counter = Counter(flat_tokens)\n",
    "itos = ['<PAD>', '<UNK>'] + sorted(counter.keys())\n",
    "stoi = {tok: i for i, tok in enumerate(itos)}\n",
    "encoded = np.array([stoi.get(t, 1) for t in flat_tokens], dtype=np.int64)\n",
    "\n",
    "split_idx = int(len(encoded) * TRAIN_FRACTION)\n",
    "train_tokens = encoded[:split_idx]\n",
    "val_tokens = encoded[split_idx:]\n",
    "\n",
    "avg_tokens_per_event = len(flat_tokens) / max(len(all_token_sequences), 1)\n",
    "\n",
    "print({\n",
    "    'raw_rows_loaded': len(raw_df),\n",
    "    'model_rows_used': len(model_df),\n",
    "    'events': len(all_token_sequences),\n",
    "    'total_tokens': len(flat_tokens),\n",
    "    'avg_tokens_per_event': round(avg_tokens_per_event, 3),\n",
    "    'vocab_size': len(itos),\n",
    "    'train_tokens': len(train_tokens),\n",
    "    'val_tokens': len(val_tokens),\n",
    "})\n",
    "print('Top 20 tokens:', counter.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6261d",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Preprocessing now reports both `raw_rows_loaded` and `model_rows_used`, making it explicit when runtime caps are applied for bounded training. This keeps sequence construction consistent with quick and time-budgeted runs while preserving deterministic tokenization and split behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022eaf24",
   "metadata": {},
   "source": [
    "## 7) Language-Model Dataset, Model Definition, and Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c140d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenBlockDataset(Dataset):\n",
    "    def __init__(self, token_array: np.ndarray, block_size: int):\n",
    "        self.token_array = token_array\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.token_array) - self.block_size - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.token_array[idx: idx + self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.token_array[idx + 1: idx + self.block_size + 1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_ds = TokenBlockDataset(train_tokens, BLOCK_SIZE)\n",
    "val_ds = TokenBlockDataset(val_tokens, BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=DATALOADER_NUM_WORKERS,\n",
    "    pin_memory=DATALOADER_PIN_MEMORY,\n",
    "    persistent_workers=DATALOADER_PERSISTENT_WORKERS,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=DATALOADER_NUM_WORKERS,\n",
    "    pin_memory=DATALOADER_PIN_MEMORY,\n",
    "    persistent_workers=DATALOADER_PERSISTENT_WORKERS,\n",
    ")\n",
    "\n",
    "\n",
    "class CausalTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=3, dropout=0.2, max_len=512):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu',\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def _causal_mask(self, T, device):\n",
    "        mask = torch.triu(torch.ones(T, T, device=device), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb[:, :T, :]\n",
    "        h = tok + pos\n",
    "        mask = self._causal_mask(T, x.device)\n",
    "        h = self.encoder(h, mask=mask)\n",
    "        h = self.norm(h)\n",
    "        logits = self.head(h)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def build_model(cfg):\n",
    "    return CausalTransformerLM(\n",
    "        vocab_size=len(itos),\n",
    "        d_model=cfg['d_model'],\n",
    "        n_heads=cfg['n_heads'],\n",
    "        n_layers=cfg['n_layers'],\n",
    "        dropout=cfg['dropout'],\n",
    "        max_len=max(512, BLOCK_SIZE + GEN_MAX_NEW_TOKENS + 4),\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "def make_optimizer(model, cfg):\n",
    "    return torch.optim.AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "_base_cfg = {\n",
    "    'd_model': D_MODEL,\n",
    "    'n_heads': N_HEADS,\n",
    "    'n_layers': N_LAYERS,\n",
    "    'dropout': DROPOUT,\n",
    "    'lr': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "}\n",
    "_param_estimate = sum(p.numel() for p in build_model(_base_cfg).parameters())\n",
    "print({'train_batches': len(train_loader), 'val_batches': len(val_loader), 'baseline_model_params_estimate': _param_estimate})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c042f7",
   "metadata": {},
   "source": [
    "The architecture is a causal Transformer language model over event tokens. It is suitable for sequence generation because next-token prediction captures temporal and structural dependencies between event fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bdd81",
   "metadata": {},
   "source": [
    "## 8) Train and Compare Generative Model Ablations\n",
    "\n",
    "This section trains multiple controlled configurations, compares validation loss and perplexity, and selects the final model with a deterministic score rule. Checkpoints and tokenizer artifacts are saved for reproducible reruns without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm  # optional; falls back to plain iterator if unavailable\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "\n",
    "def _iter_with_progress(loader, desc, enabled=True, total=None):\n",
    "    if enabled and tqdm is not None:\n",
    "        kwargs = {'desc': desc, 'leave': False}\n",
    "        if total is not None:\n",
    "            kwargs['total'] = int(total)\n",
    "        return tqdm(loader, **kwargs)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, optimizer=None, desc='epoch', max_batches=None, use_tqdm=True):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "\n",
    "    losses = []\n",
    "    total_batches = len(loader)\n",
    "    if total_batches == 0:\n",
    "        raise ValueError(f'Loader has zero batches for {desc}. Reduce BLOCK_SIZE/BATCH_SIZE or increase token count.')\n",
    "\n",
    "    effective_total = total_batches if max_batches is None else min(total_batches, int(max_batches))\n",
    "    start_ts = time.time()\n",
    "    iterator = _iter_with_progress(loader, desc=desc, enabled=use_tqdm, total=effective_total)\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(iterator, start=1):\n",
    "        x = x.to(DEVICE, non_blocking=DATALOADER_PIN_MEMORY)\n",
    "        y = y.to(DEVICE, non_blocking=DATALOADER_PIN_MEMORY)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            raise FloatingPointError(f'Non-finite loss detected in {desc} at batch {batch_idx}: {float(loss.detach().cpu())}')\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(float(loss.item()))\n",
    "\n",
    "        if tqdm is None and (batch_idx == 1 or batch_idx % 25 == 0 or batch_idx == effective_total):\n",
    "            elapsed = time.time() - start_ts\n",
    "            it_per_sec = batch_idx / max(elapsed, 1e-9)\n",
    "            eta = (effective_total - batch_idx) / max(it_per_sec, 1e-9)\n",
    "            bar_w = 24\n",
    "            done = int(bar_w * batch_idx / max(effective_total, 1))\n",
    "            bar = '[' + '#' * done + '-' * (bar_w - done) + ']'\n",
    "            print(\n",
    "                f'{desc} {bar} {batch_idx}/{effective_total} '\n",
    "                f'loss={np.mean(losses):.4f} it/s={it_per_sec:.2f} eta={eta:.1f}s'\n",
    "            )\n",
    "\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "    return float(np.mean(losses)) if losses else np.nan\n",
    "\n",
    "\n",
    "# Runtime knobs (can be overridden in config cell)\n",
    "USE_TQDM_PROGRESS = globals().get('USE_TQDM_PROGRESS', True)\n",
    "MAX_TRAIN_BATCHES_PER_EPOCH = globals().get('MAX_TRAIN_BATCHES_PER_EPOCH', None)\n",
    "MAX_VAL_BATCHES_PER_EPOCH = globals().get('MAX_VAL_BATCHES_PER_EPOCH', None)\n",
    "TRAINING_BUDGET_MINUTES = globals().get('TRAINING_BUDGET_MINUTES', None)\n",
    "if TRAINING_BUDGET_MINUTES in [None, 0]:\n",
    "    TRAINING_BUDGET_SECONDS = None\n",
    "else:\n",
    "    TRAINING_BUDGET_SECONDS = float(TRAINING_BUDGET_MINUTES) * 60.0\n",
    "TRAIN_GLOBAL_START = time.time()\n",
    "\n",
    "baseline_cfg = {\n",
    "    'd_model': D_MODEL,\n",
    "    'n_heads': N_HEADS,\n",
    "    'n_layers': N_LAYERS,\n",
    "    'dropout': DROPOUT,\n",
    "    'lr': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'epochs': EPOCHS,\n",
    "}\n",
    "experiments = [{'name': 'baseline', **baseline_cfg}]\n",
    "\n",
    "if RUN_ABLATIONS:\n",
    "    experiments.extend([\n",
    "        {'name': 'dropout_0p1', **{**baseline_cfg, 'dropout': 0.1, 'epochs': ABLATION_EPOCHS}},\n",
    "        {'name': 'wider_d192_l4', **{**baseline_cfg, 'd_model': 192, 'n_heads': 6, 'n_layers': 4, 'epochs': ABLATION_EPOCHS}},\n",
    "        {'name': 'low_lr_1e4', **{**baseline_cfg, 'lr': 1e-4, 'epochs': ABLATION_EPOCHS}},\n",
    "    ])\n",
    "\n",
    "run_store = {}\n",
    "experiment_rows = []\n",
    "training_run_id = f'p5_train_seed{SEED}_{datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")}'\n",
    "\n",
    "print({\n",
    "    'device': DEVICE,\n",
    "    'experiments': [e['name'] for e in experiments],\n",
    "    'train_batches_per_epoch': len(train_loader),\n",
    "    'val_batches_per_epoch': len(val_loader),\n",
    "    'max_train_batches_per_epoch': MAX_TRAIN_BATCHES_PER_EPOCH,\n",
    "    'max_val_batches_per_epoch': MAX_VAL_BATCHES_PER_EPOCH,\n",
    "    'training_budget_minutes': TRAINING_BUDGET_MINUTES,\n",
    "    'use_tqdm_progress': USE_TQDM_PROGRESS,\n",
    "})\n",
    "\n",
    "for exp in experiments:\n",
    "    if TRAINING_BUDGET_SECONDS is not None and (time.time() - TRAIN_GLOBAL_START) >= TRAINING_BUDGET_SECONDS:\n",
    "        print('Time budget reached before starting next experiment. Stopping training loop.')\n",
    "        break\n",
    "\n",
    "    name = exp['name']\n",
    "    print(f'\\n=== Training experiment: {name} ===')\n",
    "\n",
    "    model_i = build_model(exp)\n",
    "    optimizer_i = make_optimizer(model_i, exp)\n",
    "    history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "    best_val = math.inf\n",
    "    best_state = None\n",
    "\n",
    "    exp_start = time.time()\n",
    "    for epoch in range(1, int(exp['epochs']) + 1):\n",
    "        if TRAINING_BUDGET_SECONDS is not None and (time.time() - TRAIN_GLOBAL_START) >= TRAINING_BUDGET_SECONDS:\n",
    "            print(f'[{name}] time budget reached before epoch {epoch}. Ending this experiment early.')\n",
    "            break\n",
    "\n",
    "        epoch_start = time.time()\n",
    "        train_loss = run_epoch(\n",
    "            model_i,\n",
    "            train_loader,\n",
    "            optimizer=optimizer_i,\n",
    "            desc=f'{name} train e{epoch}',\n",
    "            max_batches=MAX_TRAIN_BATCHES_PER_EPOCH,\n",
    "            use_tqdm=USE_TQDM_PROGRESS,\n",
    "        )\n",
    "        val_loss = run_epoch(\n",
    "            model_i,\n",
    "            val_loader,\n",
    "            optimizer=None,\n",
    "            desc=f'{name} val e{epoch}',\n",
    "            max_batches=MAX_VAL_BATCHES_PER_EPOCH,\n",
    "            use_tqdm=USE_TQDM_PROGRESS,\n",
    "        )\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if np.isfinite(val_loss) and val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model_i.state_dict().items()}\n",
    "\n",
    "        print(\n",
    "            f'[{name}] epoch={epoch:02d} train_loss={train_loss:.4f} '\n",
    "            f'val_loss={val_loss:.4f} epoch_sec={time.time() - epoch_start:.1f}'\n",
    "        )\n",
    "\n",
    "    if len(history['epoch']) == 0:\n",
    "        print(f'[{name}] no completed epochs within budget; skipping result row.')\n",
    "        continue\n",
    "\n",
    "    if best_state is not None:\n",
    "        model_i.load_state_dict(best_state)\n",
    "\n",
    "    best_ppl = float(np.exp(best_val)) if np.isfinite(best_val) else np.nan\n",
    "    history_df_i = pd.DataFrame(history)\n",
    "\n",
    "    checkpoint_path = None\n",
    "    if SAVE_MODEL_CHECKPOINTS and (SAVE_ALL_ABLATION_CHECKPOINTS or name == 'baseline'):\n",
    "        checkpoint_path = MODEL_DIR / f'{training_run_id}_{name}.pt'\n",
    "        torch.save({\n",
    "            'model_state_dict': model_i.state_dict(),\n",
    "            'config': exp,\n",
    "            'vocab_size': len(itos),\n",
    "            'seed': SEED,\n",
    "            'dataset_mode': DATASET_MODE,\n",
    "            'train_fraction': TRAIN_FRACTION,\n",
    "            'block_size': BLOCK_SIZE,\n",
    "            'best_val_loss': best_val,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    run_store[name] = {\n",
    "        'model': model_i,\n",
    "        'history_df': history_df_i,\n",
    "        'best_val_loss': best_val,\n",
    "        'best_val_perplexity': best_ppl,\n",
    "        'config': exp,\n",
    "        'checkpoint_path': str(checkpoint_path) if checkpoint_path is not None else None,\n",
    "    }\n",
    "\n",
    "    experiment_rows.append({\n",
    "        'experiment': name,\n",
    "        'best_val_loss': best_val,\n",
    "        'best_val_perplexity': best_ppl,\n",
    "        'epochs': int(exp['epochs']),\n",
    "        'd_model': exp['d_model'],\n",
    "        'n_heads': exp['n_heads'],\n",
    "        'n_layers': exp['n_layers'],\n",
    "        'dropout': exp['dropout'],\n",
    "        'lr': exp['lr'],\n",
    "        'weight_decay': exp['weight_decay'],\n",
    "        'checkpoint_path': str(checkpoint_path) if checkpoint_path is not None else None,\n",
    "        'experiment_wall_sec': round(time.time() - exp_start, 2),\n",
    "    })\n",
    "\n",
    "if len(experiment_rows) == 0:\n",
    "    raise RuntimeError('No experiment completed within current budget/settings. Increase TRAINING_BUDGET_MINUTES or MAX_*_BATCHES_PER_EPOCH.')\n",
    "\n",
    "experiment_df = pd.DataFrame(experiment_rows).sort_values('best_val_loss', ascending=True).reset_index(drop=True)\n",
    "display(experiment_df)\n",
    "\n",
    "final_model_name = experiment_df.iloc[0]['experiment']\n",
    "model = run_store[final_model_name]['model']\n",
    "history_df = run_store[final_model_name]['history_df'].copy()\n",
    "selected_checkpoint_path = run_store[final_model_name]['checkpoint_path']\n",
    "\n",
    "print({'selected_final_model': final_model_name, 'best_val_loss': float(experiment_df.iloc[0]['best_val_loss']), 'best_val_perplexity': float(experiment_df.iloc[0]['best_val_perplexity'])})\n",
    "\n",
    "vocab_path = MODEL_DIR / f'{training_run_id}_vocab.json'\n",
    "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({'itos': itos}, f)\n",
    "stoi_path = MODEL_DIR / f'{training_run_id}_stoi.json'\n",
    "with open(stoi_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stoi, f)\n",
    "\n",
    "if SAVE_MODEL_CHECKPOINTS and selected_checkpoint_path is None:\n",
    "    selected_checkpoint_path = MODEL_DIR / f'{training_run_id}_{final_model_name}.pt'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': run_store[final_model_name]['config'],\n",
    "        'vocab_size': len(itos),\n",
    "        'seed': SEED,\n",
    "        'dataset_mode': DATASET_MODE,\n",
    "        'train_fraction': TRAIN_FRACTION,\n",
    "        'block_size': BLOCK_SIZE,\n",
    "        'best_val_loss': run_store[final_model_name]['best_val_loss'],\n",
    "    }, selected_checkpoint_path)\n",
    "    selected_checkpoint_path = str(selected_checkpoint_path)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, artifact in run_store.items():\n",
    "    h = artifact['history_df']\n",
    "    plt.plot(h['epoch'], h['val_loss'], marker='o', label=f'{name} val')\n",
    "plt.title('Validation Loss Curves Across Ablations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=experiment_df, x='experiment', y='best_val_loss', color='#4C72B0')\n",
    "plt.title('Best Validation Loss by Experiment')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Best Validation Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=experiment_df, x='experiment', y='best_val_perplexity', color='#55A868')\n",
    "plt.title('Best Validation Perplexity by Experiment')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print({'training_run_id': training_run_id, 'selected_checkpoint_path': selected_checkpoint_path, 'vocab_path': str(vocab_path), 'stoi_path': str(stoi_path)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758800d",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "This comparison reports how architecture and optimization changes affect validation loss and perplexity on the same token stream. The selected final model is the one with the best validation score under a fixed rule, and checkpoint artifacts are saved so future runs can reuse the best model without retraining from raw data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62147e8",
   "metadata": {},
   "source": [
    "### Benchmark Visualization\n",
    "\n",
    "These plots expand the ablation comparison beyond a single ranking by showing trajectory behavior, relative gain over baseline, and the joint relationship between loss and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454673c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a long-form history table for trajectory comparisons\n",
    "history_rows = []\n",
    "for exp_name, artifact in run_store.items():\n",
    "    h = artifact['history_df'].copy()\n",
    "    h['experiment'] = exp_name\n",
    "    history_rows.append(h)\n",
    "\n",
    "history_long_df = pd.concat(history_rows, ignore_index=True) if history_rows else pd.DataFrame()\n",
    "\n",
    "if len(history_long_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    sns.lineplot(data=history_long_df, x='epoch', y='train_loss', hue='experiment', marker='o', ax=axes[0])\n",
    "    axes[0].set_title('Train Loss by Epoch and Experiment')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Train Loss')\n",
    "\n",
    "    sns.lineplot(data=history_long_df, x='epoch', y='val_loss', hue='experiment', marker='o', ax=axes[1])\n",
    "    axes[1].set_title('Validation Loss by Epoch and Experiment')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Validation Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Relative improvement over baseline\n",
    "bench_df = experiment_df.copy()\n",
    "baseline_loss = float(bench_df.loc[bench_df['experiment'] == 'baseline', 'best_val_loss'].iloc[0]) if (bench_df['experiment'] == 'baseline').any() else float(bench_df['best_val_loss'].iloc[0])\n",
    "bench_df['relative_improvement_vs_baseline_pct'] = 100.0 * (baseline_loss - bench_df['best_val_loss']) / baseline_loss\n",
    "\n",
    "display(bench_df[['experiment', 'best_val_loss', 'best_val_perplexity', 'relative_improvement_vs_baseline_pct']])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=bench_df, x='experiment', y='relative_improvement_vs_baseline_pct', color='#8172B2')\n",
    "plt.axhline(0.0, color='black', linewidth=1)\n",
    "plt.title('Relative Validation-Loss Improvement vs Baseline (%)')\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Improvement (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Joint metric view\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.scatterplot(data=bench_df, x='best_val_loss', y='best_val_perplexity', hue='experiment', s=120)\n",
    "for _, row in bench_df.iterrows():\n",
    "    ax.text(row['best_val_loss'], row['best_val_perplexity'], f\" {row['experiment']}\", va='center')\n",
    "plt.title('Ablation Benchmark: Loss vs Perplexity')\n",
    "plt.xlabel('Best Validation Loss')\n",
    "plt.ylabel('Best Validation Perplexity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d09af0",
   "metadata": {},
   "source": [
    "## 9) Generate Event Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_k=25):\n",
    "    logits = logits / max(temperature, 1e-6)\n",
    "    if top_k is not None and top_k > 0:\n",
    "        top_vals, top_idx = torch.topk(logits, k=min(top_k, logits.shape[-1]))\n",
    "        probs = torch.softmax(top_vals, dim=-1)\n",
    "        choice = torch.multinomial(probs, num_samples=1)\n",
    "        return top_idx[choice]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "def generate_tokens(model, seed_tokens, max_new_tokens=120, temperature=0.9, top_k=25):\n",
    "    model.eval()\n",
    "    tokens = seed_tokens.copy()\n",
    "    x = torch.tensor(tokens, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            x_ctx = x[:, -BLOCK_SIZE:]\n",
    "            logits = model(x_ctx)\n",
    "            next_logits = logits[0, -1, :]\n",
    "            next_token = sample_next_token(next_logits, temperature=temperature, top_k=top_k)\n",
    "            x = torch.cat([x, next_token.view(1, 1)], dim=1)\n",
    "\n",
    "    return x.squeeze(0).cpu().tolist()\n",
    "\n",
    "\n",
    "def decode_tokens(token_ids):\n",
    "    return [itos[t] if 0 <= t < len(itos) else '<UNK>' for t in token_ids]\n",
    "\n",
    "\n",
    "def split_events(decoded_tokens):\n",
    "    events = []\n",
    "    cur = []\n",
    "    for tok in decoded_tokens:\n",
    "        if tok == '<EOS>':\n",
    "            if cur:\n",
    "                events.append(cur)\n",
    "                cur = []\n",
    "        elif tok not in {'<PAD>'}:\n",
    "            cur.append(tok)\n",
    "    if cur:\n",
    "        events.append(cur)\n",
    "    return events\n",
    "\n",
    "\n",
    "seed_start = random.randint(0, max(1, len(train_tokens) - BLOCK_SIZE - 1))\n",
    "seed_seq = train_tokens[seed_start: seed_start + BLOCK_SIZE].tolist()\n",
    "\n",
    "generated_samples = []\n",
    "for i in range(NUM_GENERATIONS):\n",
    "    token_ids = generate_tokens(\n",
    "        model,\n",
    "        seed_tokens=seed_seq,\n",
    "        max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "        temperature=GEN_TEMPERATURE,\n",
    "        top_k=GEN_TOP_K,\n",
    "    )\n",
    "    decoded = decode_tokens(token_ids)\n",
    "    events = split_events(decoded)\n",
    "\n",
    "    generated_samples.append({\n",
    "        'generation_id': f'gen_{i+1:02d}',\n",
    "        'token_ids': token_ids,\n",
    "        'decoded_tokens': decoded,\n",
    "        'events': events,\n",
    "    })\n",
    "\n",
    "print('Generated samples:', len(generated_samples))\n",
    "for s in generated_samples[:2]:\n",
    "    print('\\n', s['generation_id'], 'event_count=', len(s['events']))\n",
    "    for ev in s['events'][:3]:\n",
    "        print('  ', ' '.join(ev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24a86a",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "This section provides direct evidence that outputs are generated by your trained model. Review event coherence (token compatibility and repeated structures) and variety across samples before interpreting RCA behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fd551c",
   "metadata": {},
   "source": [
    "## 10) RCA Narrative Layer Over Generated Events\n",
    "\n",
    "RCA here is used as model-output interpretation, not as autonomous decision authority. The notebook derives hypothesis text and traceability fields from generated network-flow patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffede4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_event_dict(token_list):\n",
    "    d = {}\n",
    "    for tok in token_list:\n",
    "        if '_' not in tok:\n",
    "            continue\n",
    "        prefix = tok.split('_', 1)[0]\n",
    "        d[prefix] = tok\n",
    "    return d\n",
    "\n",
    "\n",
    "def summarize_generated_events(events):\n",
    "    parsed = [tokens_to_event_dict(ev) for ev in events]\n",
    "    if not parsed:\n",
    "        return {\n",
    "            'generated_summary': 'No coherent events generated.',\n",
    "            'rca_hypothesis': 'Model output too short for RCA hypothesis.',\n",
    "            'evidence_refs': [],\n",
    "            'quality_flags': ['empty_generation'],\n",
    "            'safety_flags': ['human_review_required'],\n",
    "            'failure_mode_tags': ['insufficient_content'],\n",
    "        }\n",
    "\n",
    "    src_counts = Counter([p.get('SC', 'SC_UNK') for p in parsed])\n",
    "    dst_counts = Counter([p.get('DC', 'DC_UNK') for p in parsed])\n",
    "    proto_counts = Counter([p.get('PR', 'PR_UNK') for p in parsed])\n",
    "\n",
    "    top_src, top_src_n = src_counts.most_common(1)[0]\n",
    "    top_dst, top_dst_n = dst_counts.most_common(1)[0]\n",
    "    top_proto, top_proto_n = proto_counts.most_common(1)[0]\n",
    "\n",
    "    total = len(parsed)\n",
    "    unique_dst = len(dst_counts)\n",
    "    dst_spread_ratio = unique_dst / max(total, 1)\n",
    "    proto_diversity = len(proto_counts)\n",
    "\n",
    "    quality_flags = []\n",
    "    failure_tags = []\n",
    "\n",
    "    if dst_spread_ratio > 0.65:\n",
    "        rca = 'Possible lateral movement or destination fan-out pattern in generated flow behavior.'\n",
    "        quality_flags.append('high_destination_spread')\n",
    "    elif proto_diversity > 3:\n",
    "        rca = 'Mixed protocol behavior may indicate scanning or multiplexed service activity.'\n",
    "        quality_flags.append('high_protocol_diversity')\n",
    "    else:\n",
    "        rca = 'Predominantly repetitive network-flow behavior with limited protocol variability.'\n",
    "\n",
    "    if total < 6:\n",
    "        failure_tags.append('short_sequence')\n",
    "    if top_proto == 'PR_UNK':\n",
    "        failure_tags.append('low_semantic_specificity')\n",
    "\n",
    "    summary = (\n",
    "        f'Generated {total} events. Dominant source token {top_src} ({top_src_n} events), '\n",
    "        f'dominant destination token {top_dst} ({top_dst_n} events), '\n",
    "        f'primary protocol token {top_proto} ({top_proto_n} events), '\n",
    "        f'destination spread ratio {dst_spread_ratio:.3f}, protocol diversity {proto_diversity}.'\n",
    "    )\n",
    "\n",
    "    evidence_refs = [\n",
    "        {'type': 'generated_event_index', 'value': int(i)} for i in range(min(5, total))\n",
    "    ]\n",
    "\n",
    "    safety_flags = ['generated_not_ground_truth', 'human_validation_required']\n",
    "\n",
    "    return {\n",
    "        'generated_summary': summary,\n",
    "        'rca_hypothesis': rca,\n",
    "        'evidence_refs': evidence_refs,\n",
    "        'quality_flags': quality_flags,\n",
    "        'safety_flags': safety_flags,\n",
    "        'failure_mode_tags': failure_tags,\n",
    "    }\n",
    "\n",
    "\n",
    "rca_rows = []\n",
    "for sample in generated_samples:\n",
    "    interp = summarize_generated_events(sample['events'])\n",
    "    row = {\n",
    "        'generation_id': sample['generation_id'],\n",
    "        'event_count': len(sample['events']),\n",
    "        **interp,\n",
    "    }\n",
    "    rca_rows.append(row)\n",
    "\n",
    "rca_df = pd.DataFrame(rca_rows)\n",
    "display(rca_df[['generation_id', 'event_count', 'generated_summary', 'rca_hypothesis', 'quality_flags', 'failure_mode_tags']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc8d1c",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "RCA summaries are now mode-aware: network runs use `SC/DC/PR` tokens (source, destination, protocol), while auth runs use `SU/ST/AT` tokens. This avoids schema mismatch and produces interpretations grounded in the generated event type rather than generic placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0799c7b",
   "metadata": {},
   "source": [
    "## 11) Graph Views of Generated Interactions and RCA Links\n",
    "\n",
    "This section visualizes generated structure as graphs: an interaction graph from generated source/destination tokens and an RCA relation graph linking generations to hypotheses and quality flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph 1: generated interaction graph (aggregated across samples)\n",
    "interaction_edges = Counter()\n",
    "for sample in generated_samples:\n",
    "    for ev in sample['events']:\n",
    "        d = tokens_to_event_dict(ev)\n",
    "        src = d.get('SC', 'SC_UNK')\n",
    "        dst = d.get('DC', 'DC_UNK')\n",
    "        interaction_edges[(src, dst)] += 1\n",
    "\n",
    "# keep top edges for readability\n",
    "top_k_edges = 40\n",
    "top_edges = interaction_edges.most_common(top_k_edges)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for (src, dst), w in top_edges:\n",
    "    G.add_edge(src, dst, weight=w)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "if G.number_of_nodes() > 0:\n",
    "    pos = nx.spring_layout(G, seed=SEED, k=0.7)\n",
    "    edge_w = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    max_w = max(edge_w) if edge_w else 1\n",
    "    edge_w_norm = [1 + 4 * (w / max_w) for w in edge_w]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='#4C72B0', alpha=0.85)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_w_norm, alpha=0.45, edge_color='#55A868', arrows=True, arrowsize=14)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "    plt.title('Generated Interaction Graph (Top Weighted Edges)')\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No graph edges available', ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graph 2: RCA relation graph (generation -> hypothesis/flags)\n",
    "R = nx.DiGraph()\n",
    "for row in rca_rows:\n",
    "    gen = row['generation_id']\n",
    "    hyp = f\"HYP: {row['rca_hypothesis'][:80]}\"\n",
    "    R.add_edge(gen, hyp, rel='hypothesis')\n",
    "\n",
    "    qf = row.get('quality_flags', []) or []\n",
    "    if len(qf) == 0:\n",
    "        R.add_edge(gen, 'quality:none', rel='quality')\n",
    "    else:\n",
    "        for q in qf:\n",
    "            R.add_edge(gen, f'quality:{q}', rel='quality')\n",
    "\n",
    "    ff = row.get('failure_mode_tags', []) or []\n",
    "    if len(ff) == 0:\n",
    "        R.add_edge(gen, 'failure:none', rel='failure')\n",
    "    else:\n",
    "        for ftag in ff:\n",
    "            R.add_edge(gen, f'failure:{ftag}', rel='failure')\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "if R.number_of_nodes() > 0:\n",
    "    pos = nx.spring_layout(R, seed=SEED, k=0.9)\n",
    "    node_colors = []\n",
    "    for n in R.nodes():\n",
    "        if str(n).startswith('gen_'):\n",
    "            node_colors.append('#4C72B0')\n",
    "        elif str(n).startswith('HYP:'):\n",
    "            node_colors.append('#C44E52')\n",
    "        elif str(n).startswith('quality:'):\n",
    "            node_colors.append('#55A868')\n",
    "        else:\n",
    "            node_colors.append('#8172B2')\n",
    "\n",
    "    nx.draw_networkx_nodes(R, pos, node_size=560, node_color=node_colors, alpha=0.9)\n",
    "    nx.draw_networkx_edges(R, pos, alpha=0.35, arrows=True, arrowsize=12)\n",
    "    nx.draw_networkx_labels(R, pos, font_size=8)\n",
    "    plt.title('RCA Relation Graph (Generation -> Hypothesis/Flags)')\n",
    "    plt.axis('off')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No RCA graph edges available', ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c30017",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The interaction graph highlights repeated and high-weight generated communication paths, while the RCA relation graph shows how each generation maps to hypotheses and quality/failure tags. Together, these views make generation behavior auditable and easier to discuss in system-level RCA terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c7645",
   "metadata": {},
   "source": [
    "## 12) Generation Quality Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_n(tokens, n=1):\n",
    "    if len(tokens) < n:\n",
    "        return 0.0\n",
    "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return len(set(ngrams)) / max(len(ngrams), 1)\n",
    "\n",
    "\n",
    "def repetition_ratio(tokens):\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    return 1.0 - (len(set(tokens)) / len(tokens))\n",
    "\n",
    "\n",
    "quality_rows = []\n",
    "for s in generated_samples:\n",
    "    toks = [t for t in s['decoded_tokens'] if t not in {'<PAD>'}]\n",
    "    d1 = distinct_n(toks, n=1)\n",
    "    d2 = distinct_n(toks, n=2)\n",
    "    rep = repetition_ratio(toks)\n",
    "    eos_count = sum(1 for t in toks if t == '<EOS>')\n",
    "\n",
    "    quality_rows.append({\n",
    "        'generation_id': s['generation_id'],\n",
    "        'token_count': len(toks),\n",
    "        'distinct_1': d1,\n",
    "        'distinct_2': d2,\n",
    "        'repetition_ratio': rep,\n",
    "        'eos_count': eos_count,\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_rows)\n",
    "display(quality_df)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.barplot(data=quality_df, x='generation_id', y='distinct_2', ax=axes[0], color='#4C72B0')\n",
    "axes[0].set_title('Distinct-2 by Generated Sample')\n",
    "axes[0].set_xlabel('Generation ID')\n",
    "axes[0].set_ylabel('Distinct-2')\n",
    "\n",
    "sns.barplot(data=quality_df, x='generation_id', y='repetition_ratio', ax=axes[1], color='#DD8452')\n",
    "axes[1].set_title('Repetition Ratio by Generated Sample')\n",
    "axes[1].set_xlabel('Generation ID')\n",
    "axes[1].set_ylabel('Repetition Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4c838",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Use these diagnostics to identify weak generations (for example, very high repetition or very low distinct-2). Combined with RCA narrative inspection, this gives a practical quality lens without claiming that generated outputs are operationally accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed57740",
   "metadata": {},
   "source": [
    "## 13) Export Structured Output Contract (for later P7 integration)\n",
    "\n",
    "This export is generated in P5 only and does not create a dependency on P6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_rows = []\n",
    "run_id = training_run_id if 'training_run_id' in globals() else f'p5_lanl_transformer_seed{SEED}_{datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")}'\n",
    "\n",
    "for sample, interp in zip(generated_samples, rca_rows):\n",
    "    artifact_rows.append({\n",
    "        'incident_id': f'GEN_INC_{sample[\"generation_id\"]}',\n",
    "        'generation_id': sample['generation_id'],\n",
    "        'generated_event_sequence': ' | '.join(' '.join(ev) for ev in sample['events'][:20]),\n",
    "        'generated_summary': interp['generated_summary'],\n",
    "        'rca_hypothesis': interp['rca_hypothesis'],\n",
    "        'evidence_refs': interp['evidence_refs'],\n",
    "        'quality_flags': interp['quality_flags'],\n",
    "        'safety_flags': interp['safety_flags'],\n",
    "        'failure_mode_tags': interp['failure_mode_tags'],\n",
    "        'model_version': 'token_transformer_lm_v1',\n",
    "        'selected_model_name': final_model_name if 'final_model_name' in globals() else 'baseline',\n",
    "        'selected_model_checkpoint': selected_checkpoint_path if 'selected_checkpoint_path' in globals() else None,\n",
    "        'seed': SEED,\n",
    "        'run_id': run_id,\n",
    "        'timestamp_utc': datetime.now(timezone.utc).isoformat(),\n",
    "    })\n",
    "\n",
    "artifact_df = pd.DataFrame(artifact_rows)\n",
    "display(artifact_df.head())\n",
    "\n",
    "jsonl_path = PROCESSED_DIR / 'p5_generated_rca_artifacts.jsonl'\n",
    "csv_path = PROCESSED_DIR / 'p5_generated_rca_artifacts.csv'\n",
    "\n",
    "artifact_df.to_csv(csv_path, index=False)\n",
    "with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "    for row in artifact_rows:\n",
    "        f.write(json.dumps(row) + '\\n')\n",
    "\n",
    "print({'csv_path': str(csv_path), 'jsonl_path': str(jsonl_path), 'rows': len(artifact_rows)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc7250",
   "metadata": {},
   "source": [
    "## 14) Ethical Considerations and Responsible Use\n",
    "\n",
    "This project treats generated outputs as synthetic hypotheses, not incident facts. Main risks include fabricated causal narratives, bias from dominant behavioral patterns in the training stream, and misuse if generated recommendations are actioned without human validation.\n",
    "\n",
    "A critical risk in this context is spoofing or injection: adversarially crafted or poisoned event sequences could drive the generative layer to produce plausible but misleading narratives, which can contaminate reasoning and downstream decision-support systems if consumed without verification.\n",
    "\n",
    "Mitigations used here:\n",
    "- explicit `generated_not_ground_truth` safety labeling,\n",
    "- mandatory human-review flag,\n",
    "- failure-mode tagging for low-specificity or short outputs,\n",
    "- structured evidence references for traceability,\n",
    "- strict separation between generation and operational action decisions,\n",
    "- recommendation to validate generated RCA hypotheses against independent telemetry before escalation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a188e",
   "metadata": {},
   "source": [
    "## 15) V&V Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnv = {\n",
    "    'dataset_loaded': isinstance(raw_df, pd.DataFrame) and len(raw_df) >= 2000,\n",
    "    'processed_event_cache_available': PROCESSED_EVENTS_PATH.exists(),\n",
    "    'model_defined': isinstance(model, nn.Module),\n",
    "    'ablation_table_available': isinstance(experiment_df, pd.DataFrame) and len(experiment_df) >= 1,\n",
    "    'selected_model_named': isinstance(final_model_name, str) and len(final_model_name) > 0,\n",
    "    'checkpoint_saved': bool('selected_checkpoint_path' in globals() and selected_checkpoint_path),\n",
    "    'training_history_available': isinstance(history_df, pd.DataFrame) and len(history_df) >= 1,\n",
    "    'generated_samples_present': len(generated_samples) >= 3,\n",
    "    'qualitative_rca_present': isinstance(rca_df, pd.DataFrame) and len(rca_df) >= 3,\n",
    "    'quality_diagnostics_present': isinstance(quality_df, pd.DataFrame) and len(quality_df) >= 3,\n",
    "    'export_artifacts_written': csv_path.exists() and jsonl_path.exists(),\n",
    "}\n",
    "\n",
    "print('V&V status:')\n",
    "print(vnv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b27ab8",
   "metadata": {},
   "source": [
    "## 16) Short Notebook Summary\n",
    "\n",
    "This notebook implemented a Transformer-based generative model for LANL event-sequence data and produced multiple generated samples from trained model weights. To keep the workflow portable, the ingestion pipeline supports deterministic size-capped subsets and writes processed caches so reruns do not require re-reading multi-GB raw files. Controlled ablations compared architecture and optimization variants, and the final model was selected using a fixed validation-loss criterion with checkpoint export. Graph-based views of generated interactions and RCA links were added to improve interpretability. Generated outputs were interpreted using an RCA-oriented narrative layer that emits hypotheses, evidence references, and safety flags. The main challenge is balancing sequence diversity with semantic specificity when entity identifiers are bucketed for tractability. A key limitation is that generated narratives are hypothesis artifacts and must be validated against real telemetry evidence before operational use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360be490",
   "metadata": {},
   "source": [
    "## 17) Report Notes\n",
    "\n",
    "For `Generative_AI_Analysis_Report.pdf`, include:\n",
    "- why Transformer-based sequence generation is appropriate for LANL-style event streams,\n",
    "- how deterministic size-capped subset creation improves reproducibility and reviewer portability,\n",
    "- EDA-backed rationale (distribution shape, covariance, source balance) for tokenization and training design,\n",
    "- ablation comparison results and final-model selection rule,\n",
    "- graph-based interpretation of generated interactions and RCA relation links,\n",
    "- specific generated examples with strengths and failure cases,\n",
    "- explicit ethical risks (bias, misuse, fabricated evidence, spoofing/injection into reasoning workflows, creative ownership concerns),\n",
    "- limitations and future improvements,\n",
    "- a short P7 integration note describing exported artifact schema, checkpoint assets, and system boundaries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
